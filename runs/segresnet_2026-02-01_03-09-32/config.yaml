data:
  raw_data_dir: dataset/raw
  train_csv: dataset/raw/train.csv
  test_csv: dataset/raw/test.csv
  train_images: dataset/raw/train_images
  train_labels: dataset/raw/train_labels
  test_images: dataset/raw/test_images
  processed_dir: dataset/processed
  train_lmdb: dataset/processed/train.lmdb
  test_lmdb: dataset/processed/test.lmdb
  label_encoding:
    background: 0
    foreground: 1
    unlabeled: 2
  val_ratio: 0.2
  seed: 42
training:
  model: segresnet
  in_channels: 1
  out_channels: 2
  epochs: 100
  batch_size: 2
  use_amp: true
  grad_accum_steps: 1
  clip_grad_norm: 1.0
  early_stopping_patience: 10
  early_stopping_metric: competition_score
  save_every_n_epochs: 5
  save_probs_every_n_epochs: auto
  keep_last_n_prob_epochs: 3
  surface_dice_tau: 2.0
  voi_alpha: 0.3
  resume_from: null
models:
  unet:
    linear_probe:
      learning_rate: 0.001
      weight_decay: 0.0001
      warmup_epochs: 3
      optimizer: adamw
      scheduler: cosine
      finetune_mode: head_only
    middle:
      learning_rate: 0.0001
      weight_decay: 1.0e-05
      warmup_epochs: 5
      optimizer: adamw
      scheduler: cosine
      finetune_mode: middle
      lr_decay_rate: 0.75
      freeze_encoder_ratio: 0.5
    full:
      learning_rate: 0.0001
      weight_decay: 1.0e-05
      warmup_epochs: 5
      optimizer: adamw
      scheduler: cosine
      finetune_mode: full
      lr_decay_rate: 0.75
  segresnet:
    linear_probe:
      learning_rate: 0.001
      weight_decay: 0.0001
      warmup_epochs: 3
      optimizer: adamw
      scheduler: cosine
      finetune_mode: head_only
    middle:
      learning_rate: 0.0001
      weight_decay: 1.0e-05
      warmup_epochs: 5
      optimizer: adamw
      scheduler: cosine
      finetune_mode: middle
      lr_decay_rate: 0.75
      freeze_encoder_ratio: 0.5
    full:
      learning_rate: 0.0001
      weight_decay: 1.0e-05
      warmup_epochs: 5
      optimizer: adamw
      scheduler: cosine
      finetune_mode: full
      lr_decay_rate: 0.75
  segformer3d:
    linear_probe:
      learning_rate: 0.0005
      weight_decay: 0.001
      warmup_epochs: 5
      optimizer: adamw
      scheduler: cosine
      finetune_mode: head_only
    middle:
      learning_rate: 5.0e-05
      weight_decay: 0.01
      warmup_epochs: 10
      optimizer: adamw
      scheduler: cosine
      finetune_mode: middle
      lr_decay_rate: 0.65
      freeze_encoder_ratio: 0.5
    full:
      learning_rate: 2.0e-05
      weight_decay: 0.01
      warmup_epochs: 10
      optimizer: adamw
      scheduler: cosine
      finetune_mode: full
      lr_decay_rate: 0.65
  swinunetr:
    linear_probe:
      learning_rate: 0.0005
      weight_decay: 0.001
      warmup_epochs: 5
      optimizer: adamw
      scheduler: cosine
      finetune_mode: head_only
    middle:
      learning_rate: 5.0e-05
      weight_decay: 0.01
      warmup_epochs: 10
      optimizer: adamw
      scheduler: cosine
      finetune_mode: middle
      lr_decay_rate: 0.7
      freeze_encoder_ratio: 0.5
    full:
      learning_rate: 2.0e-05
      weight_decay: 0.01
      warmup_epochs: 10
      optimizer: adamw
      scheduler: cosine
      finetune_mode: full
      lr_decay_rate: 0.7
augmentation:
  z_jitter_range: 5
  stats_path: dataset/stats/mean_std.npy
