data:
  # Raw data paths
  raw_data_dir: "dataset/raw"
  train_csv: "dataset/raw/train.csv"
  test_csv: "dataset/raw/test.csv"
  train_images: "dataset/raw/train_images"
  train_labels: "dataset/raw/train_labels"
  test_images: "dataset/raw/test_images"
  
  # Processed LMDB paths
  processed_dir: "dataset/processed"
  train_lmdb: "dataset/processed/train.lmdb"
  test_lmdb: "dataset/processed/test.lmdb"
  
  # Label encoding
  label_encoding:
    background: 0
    foreground: 1
    unlabeled: 2
  
  # Dataset split
  val_ratio: 0.2
  seed: 42

# Training configuration
training:
  # Model selection: unet, segresnet, segformer3d, swinunetr
  model: "segresnet"
  in_channels: 1
  out_channels: 2
  
  # General training settings
  epochs: 100
  batch_size: 2
  use_amp: true
  grad_accum_steps: 1
  clip_grad_norm: 1.0
  
  # Early stopping
  early_stopping_patience: 10
  early_stopping_metric: "competition_score"
  
  # Checkpoint & Logging
  save_every_n_epochs: 5
  save_probs_every_n_epochs: "auto"
  keep_last_n_prob_epochs: 3
  
  # Competition metrics settings
  surface_dice_tau: 2.0
  voi_alpha: 0.3
  
  # Resume training
  resume_from: null

# Model-specific hyperparameters
# Each model has settings for: linear_probe, middle, full
models:
  unet:
    linear_probe:
      learning_rate: 1.0e-3
      weight_decay: 1.0e-4
      warmup_epochs: 3
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "head_only"
    middle:
      learning_rate: 1.0e-4
      weight_decay: 1.0e-5
      warmup_epochs: 5
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "middle"
      lr_decay_rate: 0.75
      freeze_encoder_ratio: 0.5
    full:
      learning_rate: 1.0e-4
      weight_decay: 1.0e-5
      warmup_epochs: 5
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "full"
      lr_decay_rate: 0.75
  
  segresnet:
    linear_probe:
      learning_rate: 1.0e-3
      weight_decay: 1.0e-4
      warmup_epochs: 3
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "head_only"
    middle:
      learning_rate: 1.0e-4
      weight_decay: 1.0e-5
      warmup_epochs: 5
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "middle"
      lr_decay_rate: 0.75
      freeze_encoder_ratio: 0.5
    full:
      learning_rate: 1.0e-4
      weight_decay: 1.0e-5
      warmup_epochs: 5
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "full"
      lr_decay_rate: 0.75
  
  segformer3d:
    linear_probe:
      learning_rate: 5.0e-4
      weight_decay: 1.0e-3
      warmup_epochs: 5
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "head_only"
    middle:
      learning_rate: 5.0e-5
      weight_decay: 1.0e-2
      warmup_epochs: 10
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "middle"
      lr_decay_rate: 0.65
      freeze_encoder_ratio: 0.5
    full:
      learning_rate: 2.0e-5
      weight_decay: 1.0e-2
      warmup_epochs: 10
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "full"
      lr_decay_rate: 0.65
  
  swinunetr:
    linear_probe:
      learning_rate: 5.0e-4
      weight_decay: 1.0e-3
      warmup_epochs: 5
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "head_only"
    middle:
      learning_rate: 5.0e-5
      weight_decay: 1.0e-2
      warmup_epochs: 10
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "middle"
      lr_decay_rate: 0.7
      freeze_encoder_ratio: 0.5
    full:
      learning_rate: 2.0e-5
      weight_decay: 1.0e-2
      warmup_epochs: 10
      optimizer: "adamw"
      scheduler: "cosine"
      finetune_mode: "full"
      lr_decay_rate: 0.7

# Data augmentation
augmentation:
  z_jitter_range: 5
  stats_path: "dataset/stats/mean_std.npy"